{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769274c9-edc4-4d09-a467-633d265dc5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b1fa4bf-a5f9-4f0c-aec0-4dbdda197846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerSegmentation:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy='mean')\n",
    "        self.best_model = None\n",
    "        self.best_n_clusters = None\n",
    "        self.feature_matrix = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def prepare_features(self, customers_df, transactions_df):\n",
    "        # Create copies to avoid modifying original data\n",
    "        customers_df = customers_df.copy()\n",
    "        transactions_df = transactions_df.copy()\n",
    "        \n",
    "        # Customer profile features\n",
    "        customers_df['SignupDate'] = pd.to_datetime(customers_df['SignupDate'])\n",
    "        customers_df['account_age'] = (\n",
    "            pd.Timestamp.now() - customers_df['SignupDate']\n",
    "        ).dt.days\n",
    "        \n",
    "        # One-hot encode region\n",
    "        region_dummies = pd.get_dummies(customers_df['Region'], prefix='region')\n",
    "        \n",
    "        # Transaction features\n",
    "        transaction_features = transactions_df.groupby('CustomerID').agg({\n",
    "            'TransactionID': 'count',\n",
    "            'TotalValue': ['sum', 'mean', 'std'],\n",
    "            'Quantity': ['sum', 'mean'],\n",
    "            'Price': ['mean', 'max']\n",
    "        }).round(2)\n",
    "        \n",
    "        # Flatten column names\n",
    "        transaction_features.columns = [\n",
    "            'transaction_count',\n",
    "            'total_spend',\n",
    "            'avg_transaction_value',\n",
    "            'std_transaction_value',\n",
    "            'total_quantity',\n",
    "            'avg_quantity',\n",
    "            'avg_price',\n",
    "            'max_price'\n",
    "        ]\n",
    "        \n",
    "        # Reset index to make CustomerID a column\n",
    "        transaction_features = transaction_features.reset_index()\n",
    "        \n",
    "        # Calculate purchase frequency (transactions per month)\n",
    "        transaction_features['purchase_frequency'] = (\n",
    "            transaction_features['transaction_count'] / \n",
    "            (customers_df['account_age'] / 30)\n",
    "        ).round(2)\n",
    "        \n",
    "        # Merge all features\n",
    "        features_df = customers_df[['CustomerID', 'account_age']].merge(\n",
    "            region_dummies,\n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        ).merge(\n",
    "            transaction_features,\n",
    "            on='CustomerID',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Replace infinite values with NaN\n",
    "        features_df = features_df.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Store feature names\n",
    "        self.feature_names = [col for col in features_df.columns if col != 'CustomerID']\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    def preprocess_data(self, features_df):\n",
    "        # Extract features excluding CustomerID\n",
    "        X = features_df[self.feature_names].values\n",
    "        X = self.imputer.fit_transform(X)\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, customers_df, transactions_df, n_clusters=None):\n",
    "        \"\"\"Fit the clustering model\"\"\"\n",
    "        # Prepare features\n",
    "        features_df = self.prepare_features(customers_df, transactions_df)\n",
    "        \n",
    "        # Preprocess data\n",
    "        X = self.preprocess_data(features_df)\n",
    "        self.feature_matrix = X\n",
    "        \n",
    "        # Find optimal number of clusters if not specified\n",
    "        if n_clusters is None:\n",
    "            metrics_df = self.find_optimal_clusters(X)\n",
    "            self.best_n_clusters = metrics_df.loc[\n",
    "                metrics_df['db_index'].idxmin(), 'n_clusters'\n",
    "            ]\n",
    "        else:\n",
    "            self.best_n_clusters = n_clusters\n",
    "        \n",
    "        # Fit final model\n",
    "        self.best_model = KMeans(\n",
    "            n_clusters=self.best_n_clusters,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.best_model.fit(X)\n",
    "        \n",
    "        # Add cluster assignments to features DataFrame\n",
    "        features_df['Cluster'] = self.best_model.labels_\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    def find_optimal_clusters(self, X, max_clusters=10):\n",
    "        \"\"\"Find optimal number of clusters using multiple metrics\"\"\"\n",
    "        metrics = {\n",
    "            'n_clusters': [],\n",
    "            'db_index': [],\n",
    "            'silhouette': [],\n",
    "            'calinski': []\n",
    "        }\n",
    "        \n",
    "        for n_clusters in range(2, max_clusters + 1):\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            labels = kmeans.fit_predict(X)\n",
    "            \n",
    "            metrics['n_clusters'].append(n_clusters)\n",
    "            metrics['db_index'].append(davies_bouldin_index(X, labels))\n",
    "            metrics['silhouette'].append(silhouette_score(X, labels))\n",
    "            metrics['calinski'].append(calinski_harabasz_score(X, labels))\n",
    "        \n",
    "        return pd.DataFrame(metrics)\n",
    "    \n",
    "    def get_cluster_metrics(self):\n",
    "        \"\"\"Calculate clustering metrics\"\"\"\n",
    "        labels = self.best_model.labels_\n",
    "        \n",
    "        return {\n",
    "            'n_clusters': self.best_n_clusters,\n",
    "            'db_index': davies_bouldin_index(self.feature_matrix, labels),\n",
    "            'silhouette_score': silhouette_score(self.feature_matrix, labels),\n",
    "            'calinski_score': calinski_harabasz_score(self.feature_matrix, labels)\n",
    "        }\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Create visualizations of the clustering results\"\"\"\n",
    "        # Reduce dimensionality for visualization\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(self.feature_matrix)\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        \n",
    "        # Plot 1: Cluster visualization\n",
    "        plt.subplot(2, 2, 1)\n",
    "        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                            c=self.best_model.labels_, cmap='viridis')\n",
    "        plt.title('Customer Segments (PCA)')\n",
    "        plt.xlabel('First Principal Component')\n",
    "        plt.ylabel('Second Principal Component')\n",
    "        plt.colorbar(scatter)\n",
    "        \n",
    "        # Plot 2: Feature importance per cluster\n",
    "        cluster_centers = pd.DataFrame(\n",
    "            self.scaler.inverse_transform(self.best_model.cluster_centers_),\n",
    "            columns=self.feature_names\n",
    "        )\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        sns.heatmap(cluster_centers, cmap='RdYlBu', center=0)\n",
    "        plt.title('Feature Importance by Cluster')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Plot 3: Cluster sizes\n",
    "        plt.subplot(2, 2, 3)\n",
    "        cluster_sizes = pd.Series(self.best_model.labels_).value_counts()\n",
    "        cluster_sizes.plot(kind='bar')\n",
    "        plt.title('Cluster Sizes')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Number of Customers')\n",
    "        \n",
    "        # Plot 4: Feature distributions by cluster\n",
    "        plt.subplot(2, 2, 4)\n",
    "        feature_importance = np.abs(cluster_centers - cluster_centers.mean()).mean()\n",
    "        feature_importance.sort_values(ascending=True).plot(kind='barh')\n",
    "        plt.title('Overall Feature Importance')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98dc9ff9-4af5-4f70-b0c7-5774e611a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def davies_bouldin_index(X, labels):\n",
    "    n_clusters = len(np.unique(labels))\n",
    "    cluster_centers = np.array([X[labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "    \n",
    "    # Calculate cluster dispersions\n",
    "    dispersions = np.zeros(n_clusters)\n",
    "    for i in range(n_clusters):\n",
    "        if sum(labels == i) > 0:\n",
    "            cluster_points = X[labels == i]\n",
    "            dispersions[i] = np.mean(np.linalg.norm(cluster_points - cluster_centers[i], axis=1))\n",
    "    \n",
    "    # Calculate distances between cluster centers\n",
    "    center_distances = squareform(pdist(cluster_centers))\n",
    "    \n",
    "    # Calculate Davies-Bouldin Index\n",
    "    db_index = 0\n",
    "    for i in range(n_clusters):\n",
    "        if sum(labels == i) > 0:\n",
    "            ratios = np.zeros(n_clusters)\n",
    "            for j in range(n_clusters):\n",
    "                if i != j and sum(labels == j) > 0:\n",
    "                    ratios[j] = (dispersions[i] + dispersions[j]) / center_distances[i, j]\n",
    "            db_index += np.max(ratios[ratios != 0])\n",
    "    \n",
    "    return db_index / n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2a22d40-2791-43c3-badf-933a91b3eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load data\n",
    "    try:\n",
    "        customers_df = pd.read_csv(\"Customers.csv\")\n",
    "        transactions_df = pd.read_csv(\"Transactions.csv\")\n",
    "        \n",
    "        # Initialize and fit segmentation model\n",
    "        segmentation = CustomerSegmentation()\n",
    "        clustered_data = segmentation.fit(customers_df, transactions_df)\n",
    "        \n",
    "        # Get metrics\n",
    "        metrics = segmentation.get_cluster_metrics()\n",
    "        print(\"\\nClustering Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Create visualizations\n",
    "        plots = segmentation.plot_results()\n",
    "        plots.savefig('clustering_results.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save cluster assignments\n",
    "        cluster_results = clustered_data[['CustomerID', 'Cluster']]\n",
    "        cluster_results.to_csv('cluster_assignments.csv', index=False)\n",
    "        \n",
    "        print(\"\\nSegmentation completed successfully!\")\n",
    "        print(\"- Cluster assignments saved to 'cluster_assignments.csv'\")\n",
    "        print(\"- Visualizations saved to 'clustering_results.png'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        print(\"Please check your input data and file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1824d795-7372-4a58-89f7-38974a886ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\software\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\software\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\software\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\software\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\software\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\software\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\software\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\software\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\software\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering Metrics:\n",
      "n_clusters: 10.0000\n",
      "db_index: 1.4345\n",
      "silhouette_score: 0.1988\n",
      "calinski_score: 25.8122\n",
      "Figure(2000x1000)\n",
      "\n",
      "Segmentation completed successfully!\n",
      "- Cluster assignments saved to 'cluster_assignments.csv'\n",
      "- Visualizations saved to 'clustering_results.png'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
